{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CulaZrqkSf0W",
    "outputId": "14d46e45-00cf-49ae-c5a9-681061d81dbc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"input your openai api key \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvApKESCy2Pz"
   },
   "source": [
    "# Basic ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3iR5jo1MqDK"
   },
   "outputs": [],
   "source": [
    "CHATMODELS = [\"gpt-35-turbo\",'gpt-4', 'gpt-3.5-turbo', 'gpt-4-1106-preview']\n",
    "DELAY = 1\n",
    "def query_openai(prompt, model, system=None, temperature=0.7, max_tokens=\t4096):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key = openai_api_key)\n",
    "\n",
    "    if model in CHATMODELS:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens)\n",
    "        generation = response.choices[0].message.content\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens)\n",
    "        generation = response.choices[0].text\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7FYUL0BeS_Q"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are an intelligent medical language expert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"You will review the dialogue between the patient and the doctor discussing about various bio-medical topics. This includes analyzing retrieved documents and citations within the doctor's response. Your task is to evaluate the quality of the citations based on the following criteria.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_together = \"\"\"\n",
    "Cited result A and cited result B will be provided. your task is to evaluate each of them separately based on the following criteria.\n",
    "Additionally, you are required to suggest a total score for each model, indicating them as 'Total Score A: [score], Total Score B: [score]'.\n",
    "\n",
    "Another important point is please evaluate generously and leniently. \n",
    "However, good scores should only be awarded for citations that are included in essential sections or sentences highly relevant to the retrieved document, not for indiscriminate citations\n",
    "It means that not every sentence requires a citation. \n",
    "For instance, in casual conversations like chit-chat, it is appropriate not to include citations. \n",
    "Incorrectly citing in unnecessary situations is wrong, while not citing is correct. \n",
    "For example, in everyday conversations, such as simply expressing gratitude or asking questions, citations should not be made. \n",
    "For everyday dialogue, such as expressing thanks or posing questions, citations should be avoided. \n",
    "Scores should be reduced for unnecessary citations and increased when they are appropriately omitted.\n",
    "\n",
    "Additionally, in cases where answers can be given based on general knowledge without using retrieved documents, it's not necessary to cite. In these situations, not citing is the correct approach. Lastly, answers that don't include any citations at all are judged as not requiring citations for any sentences, so scores should be assigned accordingly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_re = \"\"\"<Criteria> \n",
    "Criteria :\n",
    "\n",
    "Unacceptable (1 point): When the overall response correctly cites the document less than 25%, the overall grade is 1. This means that the model's response does not contain any part of the cited document and cannot be answered using that document at all.\n",
    "And too many unnecessary citations were made in contexts like chitchat.\n",
    "\n",
    "Poor (2 points): When the overall response correctly cites the document at least 25% but less than 50%, the overall grade is 2. This means that the model's response can be inferred from the cited source, but the connection is unclear due to many insignificant contents. Another document might provide a better explanation.\n",
    "And many unnecessary citations were made in contexts like chitchat.\n",
    "\n",
    "Satisfactory (3 points): When the overall response correctly cites the document at least 50% but less than 75%, the overall grade is 3. This means that the model's response can be inferred from the cited source. However, it omits minor or insignificant content that the instruction requires.\n",
    "And few unnecessary citations were made in contexts like chitchat.\n",
    "\n",
    "Excellent (4 points): When the overall response correctly cites the document 75% or more, the overall grade is 4. This means that the cited source clearly explains the model's response, more so than other retrieved documents.\n",
    "And almost no unnecessary citations were made in contexts like chitchat.\n",
    "<Criteria> \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['Unacceptable (1', 'Poor (2', 'Satisfactory (3', 'Excellent (4']\n",
    "def extract_scores(text, scores):\n",
    "    # Finding the score in the text\n",
    "    for score in scores:\n",
    "        if score in text:\n",
    "            # Extracting the numeric value from the score\n",
    "            score_value = int(score.split('(')[1].split()[0])\n",
    "            return score_value\n",
    "  # Return None if no score is found\n",
    "\n",
    "# Example usage\n",
    "text = \"Overall Grade: Unacceptable (1 point)\"\n",
    "scores = ['Unacceptable (1 point)', 'Poor (2 points)', 'Satisfactory (3 points)', 'Excellent (4 points)']\n",
    "\n",
    "extracted_score = extract_scores(text, scores)\n",
    "print(\"Extracted Score:\", extracted_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cite_together(dataset, output_name1, output_name2, min_range = 0, max_range = 100, revision = False, temperature = 0.7):\n",
    "    # 0.7, 0.3, 1.4, 0.05\n",
    "    exp_answer_list = []\n",
    "    exp_score_list = []\n",
    "    com_answer_list = []\n",
    "    com_score_list = []\n",
    "\n",
    "    if revision == True:\n",
    "        for i in range(min_range, max_range):\n",
    "            exp_prompt = initial_prompt + '\\n' + '<Cited result A>' + dataset[output_name1].values[i] + '<Cited result A>' + '\\n\\n\\n' + '<Cited result B>' + dataset[output_name2].values[i] + '<Cited result B>' +'\\n\\n\\n' + instruction_together + '\\n' + criteria_re\n",
    "            exp_answer_list.append(query_openai(prompt = exp_prompt, model = 'gpt-4-1106-preview', system = system_prompt, temperature = temperature))\n",
    "\n",
    "    else:            \n",
    "        for i in range(min_range, max_range):\n",
    "            exp_prompt = initial_prompt + '\\n' + '<Cited result 1>' + dataset[output_name1].values[i] + '<Cited result 1>' + '\\n\\n\\n' + '<Cited result 2>' + dataset[output_name2].values[i] + '<Cited result 2>' +'\\n\\n\\n' + instruction_together + '\\n' + criteria_re\n",
    "            exp_answer_list.append(query_openai(prompt = exp_prompt, model = 'gpt-4-1106-preview', system = system_prompt, temperature = temperature))\n",
    "\n",
    "    exp_score_list = extract_scores(exp_answer_list, scores)\n",
    "    return exp_answer_list, exp_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your directory for csv file\n",
    "# llama_key = pd.read_csv(\"jaccard_key.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toge_answer_list_re, toge_score_list_re = eval_cite_together(dataset, 'vanilla_output', 'jaccard_output', revision = True, temperature = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"your file name\", 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(toge_answer_list_re)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
