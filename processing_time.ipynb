{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74495d-3a8a-4525-8256-66244ffdb35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd98db4-d23b-4aea-82db-7c5b700a4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(X, Y):\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    L = [[0] * (n + 1) for i in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                L[i][j] = 0\n",
    "            elif X[i - 1] == Y[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "    return L[m][n]\n",
    "\n",
    "\n",
    "def compute_lcs_score_matrix(per_sentence, documents):\n",
    "    num_sentences = len(per_sentence)\n",
    "    num_docs = len(documents)\n",
    "    score_matrix = torch.zeros((num_sentences, num_docs))\n",
    "\n",
    "    for i, sentence in enumerate(per_sentence):\n",
    "        for j, doc_content in enumerate(documents):\n",
    "            score_matrix[i][j] = lcs(sentence, doc_content)\n",
    "\n",
    "    return score_matrix\n",
    "\n",
    "\n",
    "def compute_score_matrix(per_sentence, documents, score_type='bleu', n_gram=1):\n",
    "    num_sentences = len(per_sentence)\n",
    "    num_docs = len(documents)\n",
    "    score_matrix = torch.zeros((num_sentences, num_docs))\n",
    "\n",
    "    for i, sentence in enumerate(per_sentence):\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        for j, doc in enumerate(documents):\n",
    "            # Ensure doc is a string before tokenization\n",
    "            doc_content = doc if isinstance(doc, str) else str(doc)\n",
    "            doc_tokens = word_tokenize(doc_content)\n",
    "\n",
    "            if score_type == 'bleu':\n",
    "                score_matrix[i][j] = calculate_bleu(doc_tokens, sentence_tokens, n_gram)\n",
    "            elif score_type == 'rouge':\n",
    "                score_matrix[i][j] = calculate_rouge(doc_tokens, sentence_tokens, n_gram)\n",
    "\n",
    "    return score_matrix\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate, n_gram):\n",
    "    weights = [1.0 / n_gram] * n_gram  # Equal weights for each n-gram\n",
    "    return sentence_bleu([reference], candidate, weights=weights)\n",
    "\n",
    "def calculate_rouge(reference, candidate, n_gram):\n",
    "    scorer = rouge_scorer.RougeScorer([f'rouge{n_gram}'], use_stemmer=True)\n",
    "    scores = scorer.score(' '.join(reference), ' '.join(candidate))\n",
    "    return scores[f'rouge{n_gram}'].fmeasure\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "def compute_jaccard_matrix(keyword_sentences, keyword_documents):\n",
    "    num_sentences = len(keyword_sentences)\n",
    "    num_docs = len(keyword_documents)\n",
    "    jaccard_matrix = torch.zeros((num_sentences, num_docs))\n",
    "\n",
    "    for i, sentence_keywords in enumerate(keyword_sentences):\n",
    "        for j, document_keywords in enumerate(keyword_documents):\n",
    "            jaccard_matrix[i][j] = jaccard_similarity(sentence_keywords, document_keywords)\n",
    "\n",
    "    return jaccard_matrix\n",
    "\n",
    "\n",
    "def flatten_keyword_list(keyword_list):\n",
    "    flattened_set = set()\n",
    "    for keywords in keyword_list:\n",
    "        if isinstance(keywords, set):\n",
    "            flattened_set = flattened_set.union(keywords)\n",
    "        elif isinstance(keywords, str):\n",
    "            flattened_set.add(keywords)\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected type in keyword list: {type(keywords)}\")\n",
    "    return flattened_set\n",
    "    \n",
    "\n",
    "def compute_tfidf_similarity_matrix(per_sentence, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    combined_texts = per_sentence + documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[:len(per_sentence)], tfidf_matrix[len(per_sentence):])\n",
    "    return similarity_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa34f0-b1cf-4ab3-a6ad-7fde24f5febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(per_sentence, documents, score_matrix, debug=False):\n",
    "    txt = ''\n",
    "    used_docs = list()\n",
    "    \n",
    "#argmax_score_matrix = torch.argmax(score_matrix, dim=-1)\n",
    "\n",
    "    if isinstance(score_matrix, np.ndarray):\n",
    "        argmax_score_matrix = np.argmax(score_matrix, axis=-1)\n",
    "    else:\n",
    "        argmax_score_matrix = torch.argmax(score_matrix, dim=-1)\n",
    "        \n",
    "    for i in range(len(per_sentence)):\n",
    "        txt += f'{per_sentence[i]}.'\n",
    "\n",
    "        if documents and 0 <= argmax_score_matrix[i] < len(documents):\n",
    "            metadata = documents[argmax_score_matrix[i]].metadata\n",
    "            page_content = documents[argmax_score_matrix[i]].page_content\n",
    "            metadata['page_content'] = page_content\n",
    "            if 'source' in metadata:\n",
    "                txt += f'\\n(Medline: {metadata[\"title\"]})'\n",
    "            else:\n",
    "                txt += f'\\n({metadata[\"first_author\"]}, {metadata[\"year\"]})'\n",
    "            if metadata not in used_docs:\n",
    "                used_docs.append(metadata)\n",
    "        else:\n",
    "            txt += '\\n(No relevant document found)'\n",
    "        txt += '\\n'\n",
    "    txt += '\\n\\n'\n",
    "\n",
    "    for x in used_docs:\n",
    "        if 'source' in x:\n",
    "            txt += f'\\nMedline {x[\"title\"]} url: {x[\"source\"]}'\n",
    "        else:\n",
    "            txt += f'\\n({x[\"first_author\"]}, {x[\"year\"]}) {x[\"title\"]} url: doi.org/{x[\"doi\"]}'\n",
    "        txt += f'\\n{x[\"page_content\"]}\\n'\n",
    "\n",
    "    return txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3cc07-ba3e-4d09-a94a-ec50d090ff70",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f193927-f0bc-49fc-961b-89e000328ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('appendix_data.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd93367-5bca-4ce6-bf68-aef67a11f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(columns = list(df.columns).extend([\"causal_output\", \"default_output\"]))\n",
    "output_df\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "keyword_docs =[]\n",
    "keyword_sents = []\n",
    "temperature = 0.07\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    output = row['generated_answer']\n",
    "    documents = row['sources']\n",
    "    \n",
    "    cur_dict = row.to_dict()\n",
    "    \n",
    "    per_sentence = [x.strip() for x in re.split('[^0-9][\".\"][^0-9]', output) if x.strip() != '']\n",
    "    document_contents = [x.page_content for x in documents]\n",
    "\n",
    "    \n",
    "    lcs_score_matrix = compute_lcs_score_matrix(per_sentence, document_contents)\n",
    "    keyword_sentences = row['keyword_sentences'][0]\n",
    "    keyword_documents = row['keyword_documents'][0]\n",
    "    # Flatten the list of sets for sentences and documents\n",
    "    flattened_sentences = [flatten_keyword_list(s) for s in keyword_sentences]\n",
    "    flattened_documents = [flatten_keyword_list(d) for d in keyword_documents]\n",
    "\n",
    "    jaccard_mat = compute_jaccard_matrix(flattened_sentences, flattened_documents)\n",
    "    bleu1_score_matrix = compute_score_matrix(per_sentence, document_contents, score_type='bleu', n_gram=1)\n",
    "    rouge1_score_matrix = compute_score_matrix(per_sentence, document_contents, score_type='rouge', n_gram=1)\n",
    "    bleu2_score_matrix = compute_score_matrix(per_sentence, document_contents, score_type='bleu', n_gram=2)\n",
    "    rouge2_score_matrix = compute_score_matrix(per_sentence, document_contents, score_type='rouge', n_gram=2)\n",
    "    tfidf_similarity_matrix = compute_tfidf_similarity_matrix(per_sentence, document_contents)\n",
    "\n",
    "    \n",
    "    cur_dict['cited_lcs'] = get_text(per_sentence, documents, lcs_score_matrix)\n",
    "    cur_dict['cited_jaccard'] = get_text(per_sentence, documents, lcs_score_matrix)\n",
    "    cur_dict['cited_bleu1'] = get_text(per_sentence, documents, bleu1_score_matrix)\n",
    "    cur_dict['cited_rouge1'] = get_text(per_sentence, documents, rouge1_score_matrix)\n",
    "    cur_dict['cited_bleu2'] = get_text(per_sentence, documents, bleu2_score_matrix)\n",
    "    cur_dict['cited_rouge2'] = get_text(per_sentence, documents, rouge2_score_matrix)\n",
    "    cur_dict['cited_tfidf'] = get_text(per_sentence, documents, tfidf_similarity_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "    output_df = pd.concat([output_df, pd.DataFrame.from_records([cur_dict])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2b902-b10e-496b-abc9-e027ba270c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_df.to_pickle('appendix_result.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fe8bb-0783-4c55-9ba0-25fa4afb4ea8",
   "metadata": {},
   "source": [
    "### Checking Processing time for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0322b-3bdb-4ef1-856e-7a7309b8a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_rows(df, num_rows, metric_function, metric_type, *args):\n",
    "    start_time = time.time()\n",
    "    for i, row in df.head(num_rows).iterrows():\n",
    "        output = row['generated_answer']\n",
    "        documents = row['sources']\n",
    "        per_sentence = [x.strip() for x in re.split('[^0-9][\".\"][^0-9]', output) if x.strip() != '']\n",
    "        document_contents = [x.page_content for x in documents]\n",
    "\n",
    "        if metric_type == 'Jaccard':\n",
    "            keyword_sentences = row['keyword_sentences'][0]  \n",
    "            keyword_documents = row['keyword_documents'][0]  \n",
    "\n",
    "            flattened_sentences = [flatten_keyword_list(s) for s in keyword_sentences]\n",
    "            flattened_documents = [flatten_keyword_list(d) for d in keyword_documents]\n",
    "\n",
    "            score_matrix = compute_jaccard_matrix(flattened_sentences, flattened_documents)\n",
    "        elif metric_type == 'LCS':\n",
    "            score_matrix = compute_lcs_score_matrix(per_sentence, document_contents)\n",
    "        elif metric_type == 'TF-IDF':\n",
    "            score_matrix = compute_tfidf_similarity_matrix(per_sentence, document_contents)\n",
    "        else:\n",
    "            score_matrix = metric_function(per_sentence, document_contents, *args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ed8e6-1502-4f68-a274-764fabef0342",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts = [9, 49, 99, 199]\n",
    "times = {'Jaccard': [], 'LCS': [], 'BLEU-1': [], 'BLEU-2': [], 'ROUGE-1': [], 'ROUGE-2': [], 'TF-IDF': []}\n",
    "\n",
    "for count in row_counts:\n",
    "    times['Jaccard'].append(process_rows(df, count, compute_jaccard_matrix, 'Jaccard'))\n",
    "    times['LCS'].append(process_rows(df, count, compute_lcs_score_matrix, 'LCS'))\n",
    "    times['BLEU-1'].append(process_rows(df, count, compute_score_matrix, 'bleu', 1))\n",
    "    times['BLEU-2'].append(process_rows(df, count, compute_score_matrix, 'bleu', 2))\n",
    "    times['ROUGE-1'].append(process_rows(df, count, compute_score_matrix, 'rouge', 1))\n",
    "    times['ROUGE-2'].append(process_rows(df, count, compute_score_matrix, 'rouge', 2))\n",
    "    times['TF-IDF'].append(process_rows(df, count, compute_tfidf_similarity_matrix, 'TF-IDF'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fad3fe-3747-4eb7-8d67-3680f165e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#times\n",
    "import math\n",
    "log_times = {metric: [math.log(time_value) for time_value in time_values] for metric, time_values in times.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86273-7b18-44f0-8b76-4c6159c62dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric, time_values in log_times.items():\n",
    "    if len(time_values) == len(row_counts):\n",
    "        plt.plot(row_counts, time_values, label=metric)\n",
    "    else:\n",
    "        print(f\"Error: Mismatched lengths for {metric}\")\n",
    "\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Processing Time (Log Scale, in seconds)\")\n",
    "plt.title('Processing Time Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('time_complexity.pdf', bbox_inches = 'tight')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
